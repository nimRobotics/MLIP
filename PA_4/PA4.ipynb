{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA_4: Feedforward Neural Network\n",
    "\n",
    "## Aim\n",
    "Train and test a Feedforward Neural Network for MNIST digit classification.\n",
    "\n",
    "## Procedure\n",
    "* Download `mnist_file.rar` which contains mnist data as a *pickle* file and read `mnist.py` for loading partial mnist data.\n",
    "* Run read `mnist.py` file which will give 1000 train and 500 test images per each class.\n",
    "* x train,y train gives the image $784\\times1$ and corresponding label for training data. Similarly, for test data.\n",
    "* Write\n",
    "1. Neural network model using library functions.\n",
    "2. Your own neural network model and train with Back propagation\n",
    "    1. On the training data and report accuracy.\n",
    "    2. Train with Five fold cross validation (4 fold training and 1 fold testing. Repeating this for 5 times changing the test fold each time) and report the average accuracy as train accuracy.\n",
    "* Test both models with the test data.\n",
    "* Find the confusion matrix and report the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data label dim: (10000,)\n",
      "Train data features dim: (10000, 784)\n",
      "Test data label dim: (5000,)\n",
      "Test data features dim:(5000, 784)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from utils import visualise\n",
    "from read_mnist import load_data\n",
    "import random\n",
    "\n",
    "y_train,x_train,y_test,x_test=load_data()\n",
    "\n",
    "print(\"Train data label dim: {}\".format(y_train.shape))\n",
    "print(\"Train data features dim: {}\".format(x_train.shape))\n",
    "print(\"Test data label dim: {}\".format(y_test.shape))\n",
    "print(\"Test data features dim:{}\".format(x_test.shape))\n",
    "\n",
    "# visualise(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############ Activation function: sigmoid No. of neurons: 32 ############\n",
      "\n",
      "Training started!\n",
      "Loss: 2.2976980059250627 Training progress: 0/1000\n",
      "Loss: 1.8916338328561102 Training progress: 100/1000\n",
      "Loss: 1.3950471903399146 Training progress: 200/1000\n",
      "Loss: 1.0668814656330095 Training progress: 300/1000\n",
      "Loss: 0.868261085654091 Training progress: 400/1000\n",
      "Loss: 0.7362683356730162 Training progress: 500/1000\n",
      "Loss: 0.6426551204596254 Training progress: 600/1000\n",
      "Loss: 0.5737353705152737 Training progress: 700/1000\n",
      "Loss: 0.5216783262541892 Training progress: 800/1000\n",
      "Loss: 0.48145599108204695 Training progress: 900/1000\n",
      "Training finished in 128.1383409500122 s\n",
      "Test Results-\n",
      " Accuracy: 0.8944 F1-Score: 0.8944, Precision: 0.8944 Recall: 0.8944\n",
      "\n",
      "############ Activation function: relu No. of neurons: 32 ############\n",
      "\n",
      "Training started!\n",
      "Loss: 2.269555119614911 Training progress: 0/1000\n",
      "Loss: 0.6242387675986449 Training progress: 100/1000\n",
      "Loss: 0.4284764079208213 Training progress: 200/1000\n",
      "Loss: 0.36111171017770066 Training progress: 300/1000\n",
      "Loss: 0.32309172002313197 Training progress: 400/1000\n",
      "Loss: 0.29654220850059526 Training progress: 500/1000\n",
      "Loss: 0.2762474864083992 Training progress: 600/1000\n",
      "Loss: 0.25974846950766367 Training progress: 700/1000\n",
      "Loss: 0.2457748162581262 Training progress: 800/1000\n",
      "Loss: 0.23347880442289415 Training progress: 900/1000\n",
      "Training finished in 117.74289751052856 s\n",
      "Test Results-\n",
      " Accuracy: 0.3912 F1-Score: 0.3912, Precision: 0.3912 Recall: 0.3912\n",
      "\n",
      "############ Activation function: tanh No. of neurons: 32 ############\n",
      "\n",
      "Training started!\n",
      "Loss: 2.297815714039517 Training progress: 0/1000\n",
      "Loss: 0.8037935708595644 Training progress: 100/1000\n",
      "Loss: 0.5207022882099225 Training progress: 200/1000\n",
      "Loss: 0.4158580124491642 Training progress: 300/1000\n",
      "Loss: 0.36120526288908106 Training progress: 400/1000\n",
      "Loss: 0.3264878670926226 Training progress: 500/1000\n",
      "Loss: 0.3016128198438327 Training progress: 600/1000\n",
      "Loss: 0.28234511365221726 Training progress: 700/1000\n",
      "Loss: 0.2666073131446428 Training progress: 800/1000\n",
      "Loss: 0.25325584886187846 Training progress: 900/1000\n",
      "Training finished in 150.12276649475098 s\n",
      "Test Results-\n",
      " Accuracy: 0.7106 F1-Score: 0.7106, Precision: 0.7106 Recall: 0.7106\n",
      "\n",
      "############ Activation function: sigmoid No. of neurons: 64 ############\n",
      "\n",
      "Training started!\n",
      "Loss: 2.3728836095253456 Training progress: 0/1000\n",
      "Loss: 1.7241328151776205 Training progress: 100/1000\n",
      "Loss: 1.1472550756147943 Training progress: 200/1000\n",
      "Loss: 0.8463646262086623 Training progress: 300/1000\n",
      "Loss: 0.6881023106662657 Training progress: 400/1000\n",
      "Loss: 0.5924568914008324 Training progress: 500/1000\n",
      "Loss: 0.5281937604976743 Training progress: 600/1000\n",
      "Loss: 0.48190210622083485 Training progress: 700/1000\n",
      "Loss: 0.4469802809664857 Training progress: 800/1000\n",
      "Loss: 0.41972671753562973 Training progress: 900/1000\n",
      "Training finished in 168.3117196559906 s\n",
      "Test Results-\n",
      " Accuracy: 0.8998 F1-Score: 0.8998, Precision: 0.8998 Recall: 0.8998\n",
      "\n",
      "############ Activation function: relu No. of neurons: 64 ############\n",
      "\n",
      "Training started!\n",
      "Loss: 2.46205134900947 Training progress: 0/1000\n",
      "Loss: 0.5452346160961845 Training progress: 100/1000\n",
      "Loss: 0.3919195300588397 Training progress: 200/1000\n",
      "Loss: 0.33410957570720823 Training progress: 300/1000\n",
      "Loss: 0.29983103214720325 Training progress: 400/1000\n",
      "Loss: 0.27539430852837027 Training progress: 500/1000\n",
      "Loss: 0.25608516942196546 Training progress: 600/1000\n",
      "Loss: 0.23979069843354883 Training progress: 700/1000\n",
      "Loss: 0.2255801369370733 Training progress: 800/1000\n",
      "Loss: 0.2129880420400791 Training progress: 900/1000\n",
      "Training finished in 142.22865653038025 s\n",
      "Test Results-\n",
      " Accuracy: 0.6416 F1-Score: 0.6416, Precision: 0.6416 Recall: 0.6416\n",
      "\n",
      "############ Activation function: tanh No. of neurons: 64 ############\n",
      "\n",
      "Training started!\n",
      "Loss: 2.486235461068752 Training progress: 0/1000\n",
      "Loss: 0.6411966675892914 Training progress: 100/1000\n",
      "Loss: 0.44876333132110613 Training progress: 200/1000\n",
      "Loss: 0.3743077580021445 Training progress: 300/1000\n",
      "Loss: 0.3321754601490864 Training progress: 400/1000\n",
      "Loss: 0.30329164891329224 Training progress: 500/1000\n",
      "Loss: 0.2812130317262196 Training progress: 600/1000\n",
      "Loss: 0.2632125909493916 Training progress: 700/1000\n",
      "Loss: 0.24793190980962904 Training progress: 800/1000\n",
      "Loss: 0.23460048669570696 Training progress: 900/1000\n",
      "Training finished in 195.36011838912964 s\n",
      "Test Results-\n",
      " Accuracy: 0.744 F1-Score: 0.744, Precision: 0.744 Recall: 0.744\n",
      "\n",
      "############ Activation function: sigmoid No. of neurons: 128 ############\n",
      "\n",
      "Training started!\n",
      "Loss: 2.4511414536847203 Training progress: 0/1000\n",
      "Loss: 1.361768291059137 Training progress: 100/1000\n",
      "Loss: 0.8774462245443572 Training progress: 200/1000\n",
      "Loss: 0.6707298021246534 Training progress: 300/1000\n",
      "Loss: 0.5630151532967453 Training progress: 400/1000\n",
      "Loss: 0.49749088463945557 Training progress: 500/1000\n",
      "Loss: 0.4534288846861791 Training progress: 600/1000\n",
      "Loss: 0.4216509844482158 Training progress: 700/1000\n",
      "Loss: 0.3975045596057303 Training progress: 800/1000\n",
      "Loss: 0.3784004123617001 Training progress: 900/1000\n",
      "Training finished in 271.2162389755249 s\n",
      "Test Results-\n",
      " Accuracy: 0.9032 F1-Score: 0.9032, Precision: 0.9032 Recall: 0.9032\n",
      "\n",
      "############ Activation function: relu No. of neurons: 128 ############\n",
      "\n",
      "Training started!\n",
      "Loss: 2.527643352946005 Training progress: 0/1000\n",
      "Loss: 0.4811591611774168 Training progress: 100/1000\n",
      "Loss: 0.3563388145136616 Training progress: 200/1000\n",
      "Loss: 0.3047357695189314 Training progress: 300/1000\n",
      "Loss: 0.2728644834221996 Training progress: 400/1000\n",
      "Loss: 0.24941941462432568 Training progress: 500/1000\n",
      "Loss: 0.2305772591318516 Training progress: 600/1000\n",
      "Loss: 0.21463090129702933 Training progress: 700/1000\n",
      "Loss: 0.20071292077836766 Training progress: 800/1000\n",
      "Loss: 0.18829770482092198 Training progress: 900/1000\n",
      "Training finished in 202.76469016075134 s\n",
      "Test Results-\n",
      " Accuracy: 0.6182 F1-Score: 0.6182, Precision: 0.6182 Recall: 0.6182\n",
      "\n",
      "############ Activation function: tanh No. of neurons: 128 ############\n",
      "\n",
      "Training started!\n",
      "Loss: 2.5389343639703488 Training progress: 0/1000\n",
      "Loss: 0.5131984031814634 Training progress: 100/1000\n",
      "Loss: 0.3843348136605855 Training progress: 200/1000\n",
      "Loss: 0.3305415509229115 Training progress: 300/1000\n",
      "Loss: 0.2978394412661658 Training progress: 400/1000\n",
      "Loss: 0.2742960989036736 Training progress: 500/1000\n",
      "Loss: 0.25569848367763487 Training progress: 600/1000\n",
      "Loss: 0.24016188117894777 Training progress: 700/1000\n",
      "Loss: 0.22670973696790178 Training progress: 800/1000\n",
      "Loss: 0.2147817443362571 Training progress: 900/1000\n",
      "Training finished in 301.1180465221405 s\n",
      "Test Results-\n",
      " Accuracy: 0.6978 F1-Score: 0.6978, Precision: 0.6978 Recall: 0.6978\n",
      "\n",
      "############ Activation function: sigmoid No. of neurons: 256 ############\n",
      "\n",
      "Training started!\n",
      "Loss: 2.580564538930428 Training progress: 0/1000\n",
      "Loss: 1.0138689173390734 Training progress: 100/1000\n",
      "Loss: 0.6771119856265364 Training progress: 200/1000\n",
      "Loss: 0.5470936515953743 Training progress: 300/1000\n",
      "Loss: 0.4781541085766841 Training progress: 400/1000\n",
      "Loss: 0.43488129997762437 Training progress: 500/1000\n",
      "Loss: 0.40480394726231006 Training progress: 600/1000\n",
      "Loss: 0.38241894853662195 Training progress: 700/1000\n",
      "Loss: 0.36492145921891295 Training progress: 800/1000\n",
      "Loss: 0.3507311061424783 Training progress: 900/1000\n",
      "Training finished in 372.57455015182495 s\n",
      "Test Results-\n",
      " Accuracy: 0.9162 F1-Score: 0.9162, Precision: 0.9162 Recall: 0.9162\n",
      "\n",
      "############ Activation function: relu No. of neurons: 256 ############\n",
      "\n",
      "Training started!\n",
      "Loss: 2.722196586738837 Training progress: 0/1000\n",
      "Loss: 0.404859005347794 Training progress: 100/1000\n",
      "Loss: 0.31175495813611853 Training progress: 200/1000\n",
      "Loss: 0.26796096085328386 Training progress: 300/1000\n",
      "Loss: 0.2390212001439132 Training progress: 400/1000\n",
      "Loss: 0.21704757548543116 Training progress: 500/1000\n",
      "Loss: 0.19926481444159858 Training progress: 600/1000\n",
      "Loss: 0.18421822585025985 Training progress: 700/1000\n",
      "Loss: 0.17110958597477263 Training progress: 800/1000\n",
      "Loss: 0.1595461440742739 Training progress: 900/1000\n",
      "Training finished in 293.32697796821594 s\n",
      "Test Results-\n",
      " Accuracy: 0.3168 F1-Score: 0.3168, Precision: 0.3168 Recall: 0.3168\n",
      "\n",
      "############ Activation function: tanh No. of neurons: 256 ############\n",
      "\n",
      "Training started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.80472378970434 Training progress: 0/1000\n",
      "Loss: 0.42582469462082784 Training progress: 100/1000\n",
      "Loss: 0.3369965408855408 Training progress: 200/1000\n",
      "Loss: 0.295382842483104 Training progress: 300/1000\n",
      "Loss: 0.2681059751763126 Training progress: 400/1000\n",
      "Loss: 0.24749224386351362 Training progress: 500/1000\n",
      "Loss: 0.23069665713292806 Training progress: 600/1000\n",
      "Loss: 0.21638732353685305 Training progress: 700/1000\n",
      "Loss: 0.2038410978627454 Training progress: 800/1000\n",
      "Loss: 0.19262277824548682 Training progress: 900/1000\n",
      "Training finished in 448.0763533115387 s\n",
      "Test Results-\n",
      " Accuracy: 0.5506 F1-Score: 0.5506, Precision: 0.5506 Recall: 0.5506\n",
      "\n",
      "############ Activation function: sigmoid No. of neurons: 32 ############\n",
      "\n",
      "Training started (validation set 1/5)!\n",
      "Loss: 2.2975906073733006 Training progress: 0/1000\n",
      "Loss: 1.8984211244502607 Training progress: 100/1000\n",
      "Loss: 1.4048436396163675 Training progress: 200/1000\n",
      "Loss: 1.0751591222637313 Training progress: 300/1000\n",
      "Loss: 0.873755529205051 Training progress: 400/1000\n",
      "Loss: 0.7397094723120292 Training progress: 500/1000\n",
      "Loss: 0.6451918341658059 Training progress: 600/1000\n",
      "Loss: 0.576157399820758 Training progress: 700/1000\n",
      "Loss: 0.524335484509604 Training progress: 800/1000\n",
      "Loss: 0.4844201875786178 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 2/5)!\n",
      "Loss: 2.2963204861817776 Training progress: 0/1000\n",
      "Loss: 1.9037190510614275 Training progress: 100/1000\n",
      "Loss: 1.408984344140048 Training progress: 200/1000\n",
      "Loss: 1.0743230947413926 Training progress: 300/1000\n",
      "Loss: 0.8726564754337195 Training progress: 400/1000\n",
      "Loss: 0.7395633552307702 Training progress: 500/1000\n",
      "Loss: 0.6454285621100524 Training progress: 600/1000\n",
      "Loss: 0.5760430113916273 Training progress: 700/1000\n",
      "Loss: 0.5234619220575201 Training progress: 800/1000\n",
      "Loss: 0.4827008886192384 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 3/5)!\n",
      "Loss: 2.2974862330560573 Training progress: 0/1000\n",
      "Loss: 1.8906052405815312 Training progress: 100/1000\n",
      "Loss: 1.391962649358864 Training progress: 200/1000\n",
      "Loss: 1.0637883969757242 Training progress: 300/1000\n",
      "Loss: 0.8644790277040634 Training progress: 400/1000\n",
      "Loss: 0.7318718616144149 Training progress: 500/1000\n",
      "Loss: 0.6381760204074888 Training progress: 600/1000\n",
      "Loss: 0.5695241584050736 Training progress: 700/1000\n",
      "Loss: 0.5178066656663525 Training progress: 800/1000\n",
      "Loss: 0.4778540875911821 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 4/5)!\n",
      "Loss: 2.302525404337499 Training progress: 0/1000\n",
      "Loss: 1.866812125912864 Training progress: 100/1000\n",
      "Loss: 1.3675944398696034 Training progress: 200/1000\n",
      "Loss: 1.0481102427925333 Training progress: 300/1000\n",
      "Loss: 0.8554649768841909 Training progress: 400/1000\n",
      "Loss: 0.72664894857218 Training progress: 500/1000\n",
      "Loss: 0.6341920991062912 Training progress: 600/1000\n",
      "Loss: 0.56539670385248 Training progress: 700/1000\n",
      "Loss: 0.5130350177993472 Training progress: 800/1000\n",
      "Loss: 0.4723477530072398 Training progress: 900/1000\n",
      "[0.885875, 0.89, 0.8895, 0.891625]\n",
      "Train Results-\n",
      "Average validation accuracy: 0.88925\n",
      "\n",
      "############ Activation function: relu No. of neurons: 32 ############\n",
      "\n",
      "Training started (validation set 1/5)!\n",
      "Loss: 2.2714380855983074 Training progress: 0/1000\n",
      "Loss: 0.6305806089385724 Training progress: 100/1000\n",
      "Loss: 0.43234706071296614 Training progress: 200/1000\n",
      "Loss: 0.3635173792138557 Training progress: 300/1000\n",
      "Loss: 0.32429305002654885 Training progress: 400/1000\n",
      "Loss: 0.29677066608171865 Training progress: 500/1000\n",
      "Loss: 0.27548559924869037 Training progress: 600/1000\n",
      "Loss: 0.25785952374524096 Training progress: 700/1000\n",
      "Loss: 0.24275111413826497 Training progress: 800/1000\n",
      "Loss: 0.22951863200184283 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 2/5)!\n",
      "Loss: 2.2682604137275533 Training progress: 0/1000\n",
      "Loss: 0.6275363524373613 Training progress: 100/1000\n",
      "Loss: 0.4291491572198756 Training progress: 200/1000\n",
      "Loss: 0.35972070659021155 Training progress: 300/1000\n",
      "Loss: 0.3202316465424904 Training progress: 400/1000\n",
      "Loss: 0.2927175909993576 Training progress: 500/1000\n",
      "Loss: 0.27162436883159685 Training progress: 600/1000\n",
      "Loss: 0.2544750527574177 Training progress: 700/1000\n",
      "Loss: 0.2399646457329909 Training progress: 800/1000\n",
      "Loss: 0.22730143328544716 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 3/5)!\n",
      "Loss: 2.267672102694217 Training progress: 0/1000\n",
      "Loss: 0.6218809290123736 Training progress: 100/1000\n",
      "Loss: 0.4271840249882733 Training progress: 200/1000\n",
      "Loss: 0.3596972185581321 Training progress: 300/1000\n",
      "Loss: 0.32093844522898535 Training progress: 400/1000\n",
      "Loss: 0.29329130710287105 Training progress: 500/1000\n",
      "Loss: 0.27169316106023467 Training progress: 600/1000\n",
      "Loss: 0.2539979344408271 Training progress: 700/1000\n",
      "Loss: 0.2390595590471416 Training progress: 800/1000\n",
      "Loss: 0.22589242400801954 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 4/5)!\n",
      "Loss: 2.274366113202284 Training progress: 0/1000\n",
      "Loss: 0.6128768596093016 Training progress: 100/1000\n",
      "Loss: 0.4154771944591921 Training progress: 200/1000\n",
      "Loss: 0.34726288770880476 Training progress: 300/1000\n",
      "Loss: 0.30874701771335084 Training progress: 400/1000\n",
      "Loss: 0.2820479780719218 Training progress: 500/1000\n",
      "Loss: 0.26147728499535233 Training progress: 600/1000\n",
      "Loss: 0.24499818030821202 Training progress: 700/1000\n",
      "Loss: 0.23115409443644638 Training progress: 800/1000\n",
      "Loss: 0.21915059848832422 Training progress: 900/1000\n",
      "[0.885875, 0.89, 0.8895, 0.891625, 0.417375, 0.397125, 0.387625, 0.41975]\n",
      "Train Results-\n",
      "Average validation accuracy: 0.647359375\n",
      "\n",
      "############ Activation function: tanh No. of neurons: 32 ############\n",
      "\n",
      "Training started (validation set 1/5)!\n",
      "Loss: 2.2969750029794467 Training progress: 0/1000\n",
      "Loss: 0.8095487724743571 Training progress: 100/1000\n",
      "Loss: 0.5252513727313469 Training progress: 200/1000\n",
      "Loss: 0.4200198937682636 Training progress: 300/1000\n",
      "Loss: 0.3646102780834258 Training progress: 400/1000\n",
      "Loss: 0.3289610329966368 Training progress: 500/1000\n",
      "Loss: 0.3030767726874222 Training progress: 600/1000\n",
      "Loss: 0.28278021713952356 Training progress: 700/1000\n",
      "Loss: 0.26603215203055564 Training progress: 800/1000\n",
      "Loss: 0.2517122053559181 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 2/5)!\n",
      "Loss: 2.2996920757870503 Training progress: 0/1000\n",
      "Loss: 0.8072363666766921 Training progress: 100/1000\n",
      "Loss: 0.5214948960698906 Training progress: 200/1000\n",
      "Loss: 0.41524944208393644 Training progress: 300/1000\n",
      "Loss: 0.35955321259200057 Training progress: 400/1000\n",
      "Loss: 0.3240681199814247 Training progress: 500/1000\n",
      "Loss: 0.29857824433053387 Training progress: 600/1000\n",
      "Loss: 0.27873720052072337 Training progress: 700/1000\n",
      "Loss: 0.26240141561277475 Training progress: 800/1000\n",
      "Loss: 0.24841062002855982 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 3/5)!\n",
      "Loss: 2.2959409579505063 Training progress: 0/1000\n",
      "Loss: 0.8003684049472347 Training progress: 100/1000\n",
      "Loss: 0.5174259555385436 Training progress: 200/1000\n",
      "Loss: 0.4124995792020966 Training progress: 300/1000\n",
      "Loss: 0.3572137075157445 Training progress: 400/1000\n",
      "Loss: 0.3216578811908166 Training progress: 500/1000\n",
      "Loss: 0.29589494964169033 Training progress: 600/1000\n",
      "Loss: 0.2757640739434359 Training progress: 700/1000\n",
      "Loss: 0.25920980622079576 Training progress: 800/1000\n",
      "Loss: 0.24509724580655937 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 4/5)!\n",
      "Loss: 2.3021710232864403 Training progress: 0/1000\n",
      "Loss: 0.7930074524687151 Training progress: 100/1000\n",
      "Loss: 0.5114071834280369 Training progress: 200/1000\n",
      "Loss: 0.4046277075967032 Training progress: 300/1000\n",
      "Loss: 0.348413232327914 Training progress: 400/1000\n",
      "Loss: 0.31247361427017667 Training progress: 500/1000\n",
      "Loss: 0.2866201887690445 Training progress: 600/1000\n",
      "Loss: 0.2665567549115483 Training progress: 700/1000\n",
      "Loss: 0.25017712071679593 Training progress: 800/1000\n",
      "Loss: 0.2363193051580656 Training progress: 900/1000\n",
      "[0.885875, 0.89, 0.8895, 0.891625, 0.417375, 0.397125, 0.387625, 0.41975, 0.694, 0.704125, 0.717375, 0.709625]\n",
      "Train Results-\n",
      "Average validation accuracy: 0.6669999999999999\n",
      "\n",
      "############ Activation function: sigmoid No. of neurons: 64 ############\n",
      "\n",
      "Training started (validation set 1/5)!\n",
      "Loss: 2.3738032523235635 Training progress: 0/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.7328187191197506 Training progress: 100/1000\n",
      "Loss: 1.1573254648505793 Training progress: 200/1000\n",
      "Loss: 0.853451498906896 Training progress: 300/1000\n",
      "Loss: 0.6934366318692301 Training progress: 400/1000\n",
      "Loss: 0.5970959946023467 Training progress: 500/1000\n",
      "Loss: 0.532611456634444 Training progress: 600/1000\n",
      "Loss: 0.486296952445222 Training progress: 700/1000\n",
      "Loss: 0.45142393513117374 Training progress: 800/1000\n",
      "Loss: 0.4242281466510141 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 2/5)!\n",
      "Loss: 2.3746272765926624 Training progress: 0/1000\n",
      "Loss: 1.7344494576374465 Training progress: 100/1000\n",
      "Loss: 1.1539318163224006 Training progress: 200/1000\n",
      "Loss: 0.8487006560182283 Training progress: 300/1000\n",
      "Loss: 0.6891887947493998 Training progress: 400/1000\n",
      "Loss: 0.5930760820113369 Training progress: 500/1000\n",
      "Loss: 0.5284390113660795 Training progress: 600/1000\n",
      "Loss: 0.4817435497931191 Training progress: 700/1000\n",
      "Loss: 0.446407488396133 Training progress: 800/1000\n",
      "Loss: 0.4187611135925704 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 3/5)!\n",
      "Loss: 2.372996540187739 Training progress: 0/1000\n",
      "Loss: 1.7230830113106963 Training progress: 100/1000\n",
      "Loss: 1.1445272037353091 Training progress: 200/1000\n",
      "Loss: 0.8439370011165354 Training progress: 300/1000\n",
      "Loss: 0.686128630885831 Training progress: 400/1000\n",
      "Loss: 0.5906943275713485 Training progress: 500/1000\n",
      "Loss: 0.5265180231986253 Training progress: 600/1000\n",
      "Loss: 0.48026180859777917 Training progress: 700/1000\n",
      "Loss: 0.44533062093421816 Training progress: 800/1000\n",
      "Loss: 0.41801527633285435 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 4/5)!\n",
      "Loss: 2.3683329013879915 Training progress: 0/1000\n",
      "Loss: 1.695633838103867 Training progress: 100/1000\n",
      "Loss: 1.1244450343816135 Training progress: 200/1000\n",
      "Loss: 0.8323757102781496 Training progress: 300/1000\n",
      "Loss: 0.6770733882265688 Training progress: 400/1000\n",
      "Loss: 0.5820781073406868 Training progress: 500/1000\n",
      "Loss: 0.5177018176677138 Training progress: 600/1000\n",
      "Loss: 0.47104396086799455 Training progress: 700/1000\n",
      "Loss: 0.4356724729833324 Training progress: 800/1000\n",
      "Loss: 0.4079561065576223 Training progress: 900/1000\n",
      "[0.885875, 0.89, 0.8895, 0.891625, 0.417375, 0.397125, 0.387625, 0.41975, 0.694, 0.704125, 0.717375, 0.709625, 0.892625, 0.896125, 0.89575, 0.89925]\n",
      "Train Results-\n",
      "Average validation accuracy: 0.724234375\n",
      "\n",
      "############ Activation function: relu No. of neurons: 64 ############\n",
      "\n",
      "Training started (validation set 1/5)!\n",
      "Loss: 2.464689941157207 Training progress: 0/1000\n",
      "Loss: 0.5517356679269606 Training progress: 100/1000\n",
      "Loss: 0.39714964213350235 Training progress: 200/1000\n",
      "Loss: 0.3383745403962608 Training progress: 300/1000\n",
      "Loss: 0.3028165197486539 Training progress: 400/1000\n",
      "Loss: 0.2771866843309684 Training progress: 500/1000\n",
      "Loss: 0.25674912185436505 Training progress: 600/1000\n",
      "Loss: 0.23937096382884365 Training progress: 700/1000\n",
      "Loss: 0.22410426777113968 Training progress: 800/1000\n",
      "Loss: 0.21067372440357632 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 2/5)!\n",
      "Loss: 2.460346650655439 Training progress: 0/1000\n",
      "Loss: 0.5460034019637433 Training progress: 100/1000\n",
      "Loss: 0.39005028719533275 Training progress: 200/1000\n",
      "Loss: 0.33105918394426653 Training progress: 300/1000\n",
      "Loss: 0.295874206925518 Training progress: 400/1000\n",
      "Loss: 0.2706726175384152 Training progress: 500/1000\n",
      "Loss: 0.2505876736774841 Training progress: 600/1000\n",
      "Loss: 0.23358213108463907 Training progress: 700/1000\n",
      "Loss: 0.2186314973544535 Training progress: 800/1000\n",
      "Loss: 0.20535050093458546 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 3/5)!\n",
      "Loss: 2.4584614181660043 Training progress: 0/1000\n",
      "Loss: 0.5433059312977795 Training progress: 100/1000\n",
      "Loss: 0.3894279660675077 Training progress: 200/1000\n",
      "Loss: 0.3302955115890171 Training progress: 300/1000\n",
      "Loss: 0.2948738769138677 Training progress: 400/1000\n",
      "Loss: 0.26953088475264475 Training progress: 500/1000\n",
      "Loss: 0.24941468898517494 Training progress: 600/1000\n",
      "Loss: 0.23237615128015685 Training progress: 700/1000\n",
      "Loss: 0.21749253990021497 Training progress: 800/1000\n",
      "Loss: 0.20435528171893014 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 4/5)!\n",
      "Loss: 2.4639831998366453 Training progress: 0/1000\n",
      "Loss: 0.5319790807996199 Training progress: 100/1000\n",
      "Loss: 0.37862633014834707 Training progress: 200/1000\n",
      "Loss: 0.3200674521807681 Training progress: 300/1000\n",
      "Loss: 0.28525538936481937 Training progress: 400/1000\n",
      "Loss: 0.2601732623184379 Training progress: 500/1000\n",
      "Loss: 0.2404450036681326 Training progress: 600/1000\n",
      "Loss: 0.22394502774031536 Training progress: 700/1000\n",
      "Loss: 0.20959264398268576 Training progress: 800/1000\n",
      "Loss: 0.1968545068264842 Training progress: 900/1000\n",
      "[0.885875, 0.89, 0.8895, 0.891625, 0.417375, 0.397125, 0.387625, 0.41975, 0.694, 0.704125, 0.717375, 0.709625, 0.892625, 0.896125, 0.89575, 0.89925, 0.64475, 0.6275, 0.630625, 0.633125]\n",
      "Train Results-\n",
      "Average validation accuracy: 0.7061875\n",
      "\n",
      "############ Activation function: tanh No. of neurons: 64 ############\n",
      "\n",
      "Training started (validation set 1/5)!\n",
      "Loss: 2.4862241641004243 Training progress: 0/1000\n",
      "Loss: 0.646184074878039 Training progress: 100/1000\n",
      "Loss: 0.45234167343797854 Training progress: 200/1000\n",
      "Loss: 0.3776798958861662 Training progress: 300/1000\n",
      "Loss: 0.3351626257251461 Training progress: 400/1000\n",
      "Loss: 0.30570181297812005 Training progress: 500/1000\n",
      "Loss: 0.2829651345834308 Training progress: 600/1000\n",
      "Loss: 0.26429453548111465 Training progress: 700/1000\n",
      "Loss: 0.2483646768821933 Training progress: 800/1000\n",
      "Loss: 0.2344197520317913 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 2/5)!\n",
      "Loss: 2.4834795251995336 Training progress: 0/1000\n",
      "Loss: 0.640886323797289 Training progress: 100/1000\n",
      "Loss: 0.4467875051475563 Training progress: 200/1000\n",
      "Loss: 0.3707394009682183 Training progress: 300/1000\n",
      "Loss: 0.3274232267172905 Training progress: 400/1000\n",
      "Loss: 0.2976211066644753 Training progress: 500/1000\n",
      "Loss: 0.27480360314640234 Training progress: 600/1000\n",
      "Loss: 0.25618042602607866 Training progress: 700/1000\n",
      "Loss: 0.24034754139201187 Training progress: 800/1000\n",
      "Loss: 0.22651018680229942 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 3/5)!\n",
      "Loss: 2.485255194497843 Training progress: 0/1000\n",
      "Loss: 0.6402517953486158 Training progress: 100/1000\n",
      "Loss: 0.44722154838512235 Training progress: 200/1000\n",
      "Loss: 0.3718531517625554 Training progress: 300/1000\n",
      "Loss: 0.32877908988305576 Training progress: 400/1000\n",
      "Loss: 0.29906778273858636 Training progress: 500/1000\n",
      "Loss: 0.276280520115214 Training progress: 600/1000\n",
      "Loss: 0.2576578773606431 Training progress: 700/1000\n",
      "Loss: 0.24181519215825745 Training progress: 800/1000\n",
      "Loss: 0.22796913664849697 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 4/5)!\n",
      "Loss: 2.4920715946086593 Training progress: 0/1000\n",
      "Loss: 0.6296258302426344 Training progress: 100/1000\n",
      "Loss: 0.4371636068906537 Training progress: 200/1000\n",
      "Loss: 0.36159411997115226 Training progress: 300/1000\n",
      "Loss: 0.3184993948820331 Training progress: 400/1000\n",
      "Loss: 0.2888928992839356 Training progress: 500/1000\n",
      "Loss: 0.2662866673116875 Training progress: 600/1000\n",
      "Loss: 0.2478932060756191 Training progress: 700/1000\n",
      "Loss: 0.23231197122080152 Training progress: 800/1000\n",
      "Loss: 0.21875088055143108 Training progress: 900/1000\n",
      "[0.885875, 0.89, 0.8895, 0.891625, 0.417375, 0.397125, 0.387625, 0.41975, 0.694, 0.704125, 0.717375, 0.709625, 0.892625, 0.896125, 0.89575, 0.89925, 0.64475, 0.6275, 0.630625, 0.633125, 0.693875, 0.731, 0.715375, 0.738]\n",
      "Train Results-\n",
      "Average validation accuracy: 0.7084166666666666\n",
      "\n",
      "############ Activation function: sigmoid No. of neurons: 128 ############\n",
      "\n",
      "Training started (validation set 1/5)!\n",
      "Loss: 2.450830304092339 Training progress: 0/1000\n",
      "Loss: 1.3704002630006855 Training progress: 100/1000\n",
      "Loss: 0.8842045029996447 Training progress: 200/1000\n",
      "Loss: 0.676096696259144 Training progress: 300/1000\n",
      "Loss: 0.568078864562552 Training progress: 400/1000\n",
      "Loss: 0.5025304511030594 Training progress: 500/1000\n",
      "Loss: 0.4584627215028223 Training progress: 600/1000\n",
      "Loss: 0.42663927199634055 Training progress: 700/1000\n",
      "Loss: 0.40240416978865173 Training progress: 800/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3831764125814715 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 2/5)!\n",
      "Loss: 2.4561111589591724 Training progress: 0/1000\n",
      "Loss: 1.3701274093307332 Training progress: 100/1000\n",
      "Loss: 0.8814720957398179 Training progress: 200/1000\n",
      "Loss: 0.6731211176913239 Training progress: 300/1000\n",
      "Loss: 0.5644958756242395 Training progress: 400/1000\n",
      "Loss: 0.4982149466036488 Training progress: 500/1000\n",
      "Loss: 0.45353158629293255 Training progress: 600/1000\n",
      "Loss: 0.4212610475836121 Training progress: 700/1000\n",
      "Loss: 0.3967240836220287 Training progress: 800/1000\n",
      "Loss: 0.37730225574539394 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 3/5)!\n",
      "Loss: 2.455722920333523 Training progress: 0/1000\n",
      "Loss: 1.3598602703270095 Training progress: 100/1000\n",
      "Loss: 0.8746763453389527 Training progress: 200/1000\n",
      "Loss: 0.6676085185184709 Training progress: 300/1000\n",
      "Loss: 0.559886136265555 Training progress: 400/1000\n",
      "Loss: 0.49444416775111916 Training progress: 500/1000\n",
      "Loss: 0.4504307212524079 Training progress: 600/1000\n",
      "Loss: 0.4186397269006538 Training progress: 700/1000\n",
      "Loss: 0.39442446833244255 Training progress: 800/1000\n",
      "Loss: 0.3752090313361291 Training progress: 900/1000\n",
      "\n",
      "Training started (validation set 4/5)!\n",
      "Loss: 2.4425099525830594 Training progress: 0/1000\n",
      "Loss: 1.3387173813580595 Training progress: 100/1000\n",
      "Loss: 0.8634798073127409 Training progress: 200/1000\n",
      "Loss: 0.660666386212998 Training progress: 300/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e13705ccf07c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss: {} Training progress: {}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e13705ccf07c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_train)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error: Activation not implemented'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from read_mnist import load_data\n",
    "import pickle\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def tanh_grad(x):\n",
    "    return 1-np.power(x,2)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x /np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def relu_grad(x):\n",
    "    return (x>0)*1\n",
    "\n",
    "def cross_entropy(y_,y):\n",
    "    n = y.shape[0]\n",
    "    nll = -np.log(y_[range(n),y])\n",
    "    return np.mean(nll)\n",
    "\n",
    "def delta_cross_entropy(y_,y):\n",
    "    n = y.shape[0]\n",
    "    y_[range(n),y] -= 1\n",
    "    return y_/n\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, hidden_layers, hidden_neurons, hidden_activation, lr=0.01):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.lr=lr\n",
    "        np.random.seed(786)\n",
    "        self.W1 = 0.1* np.random.randn(x_train.shape[1],self.hidden_neurons)\n",
    "        self.b1 = np.zeros((1,self.hidden_neurons))\n",
    "        self.W2 = 0.1* np.random.randn(self.hidden_neurons,10)\n",
    "        self.b2 = np.zeros((1,10))\n",
    "\n",
    "    def forward(self,x_train):\n",
    "        s1=np.dot(x_train, self.W1) + self.b1\n",
    "        if self.hidden_activation == 'sigmoid':\n",
    "            a1 = sigmoid(s1)\n",
    "        elif self.hidden_activation=='tanh':\n",
    "            a1 = np.tanh(s1)\n",
    "        elif self.hidden_activation=='relu':\n",
    "            a1 = relu(s1)\n",
    "        else:\n",
    "            raise Exception('Error: Activation not implemented')\n",
    "        s2 = np.dot(a1, self.W2) + self.b2\n",
    "        a2 = softmax(s2)\n",
    "        loss=cross_entropy(a2,y_train)\n",
    "        return(loss,s1,a1,s2,a2)\n",
    "\n",
    "\n",
    "    def backward(self, s1, a1, s2, a2):\n",
    "        delta3=delta_cross_entropy(a2,y_train)\n",
    "        dW2 = np.dot(a1.T, delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        if self.hidden_activation=='sigmoid':\n",
    "            delta2 = delta3.dot(self.W2.T) * sigmoid_grad(a1)\n",
    "        elif self.hidden_activation == 'tanh':\n",
    "            delta2 = delta3.dot(self.W2.T) * tanh_grad(a1)\n",
    "        elif self.hidden_activation == 'relu':\n",
    "            delta2 = delta3.dot(self.W2.T) * relu_grad(a1)\n",
    "        else:\n",
    "            raise Exception('Error: Activation not implemented')\n",
    "            \n",
    "        dW1 = np.dot(x_train.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "        self.W1 += -self.lr * dW1\n",
    "        self.b1 += -self.lr * db1\n",
    "        self.W2 += -self.lr * dW2\n",
    "        self.b2 += -self.lr * db2\n",
    "        \n",
    "    def predict(self, x):\n",
    "        s1=np.dot(x, self.W1)\n",
    "        a1 = (sigmoid(s1))\n",
    "        s2 = np.dot(a1, self.W2)\n",
    "        a2 = softmax(s2)\n",
    "        return np.argmax(a2, axis=1)\n",
    "    \n",
    "    def save_model(self, name):\n",
    "        params = { 'W1': self.W1, 'b1': self.b1, 'W2': self.W2, 'b2': self.b2}\n",
    "        with open(name, 'wb') as handle:\n",
    "            pickle.dump(params, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "epochs=1000\n",
    "# hyperparameter variation\n",
    "lr=0.1\n",
    "neurons = [32,64,128,256]\n",
    "activations = ['sigmoid', 'relu', 'tanh']\n",
    "\n",
    "experiments = list(itertools.product(neurons, activations))\n",
    "\n",
    "for (hidden_neurons,hidden_activation) in experiments:\n",
    "    print('\\n############ Activation function: {} No. of neurons: {} ############'.format(hidden_activation, hidden_neurons))\n",
    "    model=NN(hidden_layers=5,hidden_neurons=hidden_neurons,hidden_activation=hidden_activation, lr=lr)\n",
    "    print('\\nTraining started!')\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss,s1,a1,s2,a2 = model.forward(x_train)\n",
    "        if epoch%100==0:\n",
    "            print(\"Loss: {} Training progress: {}/{}\".format(loss,epoch,epochs))\n",
    "        model.backward(s1, a1, s2, a2)\n",
    "        \n",
    "    name = 'model_'+str(hidden_activation)+'_'+str(hidden_neurons)+'.pickle'\n",
    "    model.save_model(name=name)\n",
    "    stop = time.time()\n",
    "    \n",
    "    print('Training finished in {} s'.format(stop - start))\n",
    "    test_preds = model.predict(x_test)\n",
    "    print('Test Results-\\n Accuracy: {} F1-Score: {}, Precision: {} Recall: {}'.format( np.mean(test_preds == y_test), f1_score(y_test, test_preds, average='micro'), precision_score(y_test, test_preds, average='micro'), recall_score(y_test, test_preds, average='micro') ))\n",
    "\n",
    "# copying the original data    \n",
    "y_train_or = np.copy(y_train)\n",
    "x_train_or = np.copy(x_train)\n",
    "y_test_or = np.copy(y_test)\n",
    "x_test_or = np.copy(x_test)\n",
    "\n",
    "# five fold cross validation\n",
    "folds=5\n",
    "val_acc=[]\n",
    "for (hidden_neurons,hidden_activation) in experiments:\n",
    "    print('\\n############ Activation function: {} No. of neurons: {} ############'.format(hidden_activation, hidden_neurons))\n",
    "\n",
    "    for fold in range(1,folds):\n",
    "        # x_train.shape[0]=10000\n",
    "        start=int(fold*(x_train_or.shape[0]/folds))\n",
    "        stop=int((fold+1)*(x_train_or.shape[0]/folds))\n",
    "        \n",
    "        del x_train\n",
    "        del y_train\n",
    "        del x_test\n",
    "        del y_test\n",
    "\n",
    "        x_test=x_train_or[start:stop]\n",
    "        y_test=y_train_or[start:stop]\n",
    "        \n",
    "\n",
    "        x_train=np.vstack((x_train_or[:start],  x_train_or[stop:]))\n",
    "        y_train=np.append(y_train_or[:start],y_train_or[stop:])\n",
    "        # print(x_train.shape, y_train.shape)\n",
    "\n",
    "        model=NN(hidden_layers=5,hidden_neurons=hidden_neurons,hidden_activation=hidden_activation, lr=lr)\n",
    "        print('\\nTraining started (validation set {}/{})!'.format(fold,folds))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            loss,s1,a1,s2,a2 = model.forward(x_train)\n",
    "            if epoch%100==0:\n",
    "                print(\"Loss: {} Training progress: {}/{}\".format(loss,epoch,epochs))\n",
    "            model.backward(s1, a1, s2, a2)\n",
    "\n",
    "        train_preds= model.predict(x_train)\n",
    "        val_acc.append(np.mean(train_preds == y_train))\n",
    "    print(val_acc)\n",
    "    print('Train Results-\\nAverage validation accuracy: {}'.format(np.mean(np.array(val_acc)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model parameters\n",
    "for (hidden_neurons,hidden_activation) in experiments:\n",
    "    name = 'model_'+str(hidden_activation)+'_'+str(hidden_neurons)+'.pickle'\n",
    "    print(name)\n",
    "    with open(name, 'rb') as handle:\n",
    "        b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
